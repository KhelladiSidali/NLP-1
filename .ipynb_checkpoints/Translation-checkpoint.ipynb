{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e4a71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c68335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('words')\n",
    "# nltk.download(\"punkt\")\n",
    "# nltk.download('maxent_ne_chunker')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3c0be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"NLTK_DATA\"] = \"C:/Users/Utilisateur/AppData/Roaming/nltk_data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a082364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "input_file = 'sample-0001.xml'\n",
    "output_file = 'sample.txt'\n",
    "translation_output_file = 'sample_translated.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6afd2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Preprocessing the Dataset\n",
    "def extract_abstract_texts(file_path):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    abstracts = []\n",
    "    for abstract in root.findall(\".//AbstractText\"):\n",
    "        abstracts.append(abstract.text)\n",
    "    return abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da416471",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def segment_sentences(abstracts):\n",
    "    sentences = []\n",
    "    for abstract in abstracts:\n",
    "        if abstract:\n",
    "            sentences.extend(sent_tokenize(abstract))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a66d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = extract_abstract_texts(input_file)\n",
    "sentences = segment_sentences(abstracts[:10])  # Adjust the number 10 based on your capacity\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for sentence in sentences:\n",
    "        f.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed64206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Part of Speech Tagging\n",
    "\n",
    "def pos_tagging(sentences):\n",
    "    pos_tags = [nltk.pos_tag(word_tokenize(sentence)) for sentence in sentences]\n",
    "    return pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43973699",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_transition_matrices(tags):\n",
    "    unigram_counts = Counter()\n",
    "    bigram_counts = Counter()\n",
    "    \n",
    "    for tag_seq in tags:\n",
    "        tag_seq = [tag for word, tag in tag_seq]  # Correcting the list comprehension to unpack tuples correctly\n",
    "        unigram_counts.update(tag_seq)\n",
    "        bigram_counts.update(nltk.bigrams(tag_seq))\n",
    "    \n",
    "    unigram_total = sum(unigram_counts.values())\n",
    "    bigram_total = sum(bigram_counts.values())\n",
    "    \n",
    "    unigram_probs = {tag: count/unigram_total for tag, count in unigram_counts.items()}\n",
    "    bigram_probs = {bigram: count/bigram_total for bigram, count in bigram_counts.items()}\n",
    "    \n",
    "    return unigram_probs, bigram_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd8d4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags = pos_tagging(sentences)\n",
    "pos_unigram_probs, pos_bigram_probs = compute_transition_matrices(pos_tags)\n",
    "\n",
    "print(\"### Part of Speech Tagging Unigram Probabilities:\")\n",
    "print(pos_unigram_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6366f880",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n### Part of Speech Tagging Bigram Probabilities:\")\n",
    "print(pos_bigram_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ca9b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Named Entity Recognition\n",
    "\n",
    "def ner_tagging(sentences):\n",
    "    ner_tags = []\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        tagged_words = nltk.pos_tag(words)\n",
    "        chunks = nltk.ne_chunk(tagged_words)\n",
    "        for chunk in chunks:\n",
    "            if hasattr(chunk, 'label'):\n",
    "                ner_tags.append(chunk.label())\n",
    "            else:\n",
    "                ner_tags.append(chunk[1])\n",
    "    return ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9df2ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_ner_transition_matrices(ner_tags):\n",
    "    unigram_counts = Counter()\n",
    "    bigram_counts = Counter()\n",
    "    \n",
    "    unigram_counts.update(ner_tags)\n",
    "    bigram_counts.update(nltk.bigrams(ner_tags))\n",
    "    \n",
    "    unigram_total = sum(unigram_counts.values())\n",
    "    bigram_total = sum(bigram_counts.values())\n",
    "    \n",
    "    unigram_probs = {tag: count/unigram_total for tag, count in unigram_counts.items()}\n",
    "    bigram_probs = {bigram: count/bigram_total for bigram, count in bigram_counts.items()}\n",
    "    \n",
    "    return unigram_probs, bigram_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a515adb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tags = ner_tagging(sentences)\n",
    "ner_unigram_probs, ner_bigram_probs = compute_ner_transition_matrices(ner_tags)\n",
    "\n",
    "print(\"\\n### Named Entity Recognition Unigram Probabilities:\")\n",
    "print(ner_unigram_probs)\n",
    "print(\"\\n### Named Entity Recognition Bigram Probabilities:\")\n",
    "print(ner_bigram_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900b91d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Topics Extraction Using TF-IDF for Each Abstract\n",
    "def extract_topics_for_each_abstract(abstracts, num_topics=3, num_words=5):\n",
    "    topics_per_abstract = []\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=100)\n",
    "\n",
    "    for abstract in abstracts:\n",
    "        if abstract:  # Check if the abstract is not None or empty\n",
    "            tfidf_matrix = vectorizer.fit_transform([abstract])\n",
    "            feature_names = vectorizer.get_feature_names_out()\n",
    "            \n",
    "            # Sum the TF-IDF scores for each term in the abstract\n",
    "            tfidf_scores = tfidf_matrix.sum(axis=0).A1\n",
    "            tfidf_scores = dict(zip(feature_names, tfidf_scores))\n",
    "            \n",
    "            # Sort terms by their TF-IDF scores in descending order\n",
    "            sorted_terms = sorted(tfidf_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            # Extract the top terms as topics for the current abstract\n",
    "            topics = [term for term, score in sorted_terms[:num_topics]]\n",
    "            topics_per_abstract.append(topics)\n",
    "        else:\n",
    "            topics_per_abstract.append([])  # Handle empty abstracts\n",
    "\n",
    "    return topics_per_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b344532d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topics_per_abstract = extract_topics_for_each_abstract(abstracts[:10])  # Adjust the number 10 based on your capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4df2a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n### Topics per Abstract:\")\n",
    "for i, topics in enumerate(topics_per_abstract, 1):\n",
    "    print(f\"Abstract {i}: {topics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db4ce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 5: Translation\n",
    "def translate_sentences(sentences, target_language='fr'):\n",
    "    if target_language == 'fr':\n",
    "        translator = pipeline('translation_en_to_fr', model='Helsinki-NLP/opus-mt-en-fr')\n",
    "    elif target_language == 'ar':\n",
    "        translator = pipeline('translation_en_to_ar', model='Helsinki-NLP/opus-mt-en-ar')\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported target language. Use 'fr' for French or 'ar' for Arabic.\")\n",
    "    \n",
    "    translations = translator(sentences)\n",
    "    translated_sentences = [t['translation_text'] for t in translations]\n",
    "    return translated_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6cb127",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example: Translate to French\n",
    "translated_sentences = translate_sentences(sentences, target_language='fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eda6d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(translation_output_file, 'w', encoding='utf-8') as f:\n",
    "    for sentence in translated_sentences:\n",
    "        f.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62280a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"\\n### Translated Sentences:\")\n",
    "for i, sentence in enumerate(translated_sentences, 1):\n",
    "    print(f\"Sentence {i}: {sentence}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416a083b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7576d66d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef7720d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
