{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49678de6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting googletrans==4.0.0-rc1\n",
      "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
      "  Obtaining dependency information for httpx==0.13.3 from https://files.pythonhosted.org/packages/54/b4/698b284c6aed4d7c2b4fe3ba5df1fcf6093612423797e76fbb24890dd22f/httpx-0.13.3-py3-none-any.whl.metadata\n",
      "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\utilisateur\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.2.2)\n",
      "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Obtaining dependency information for hstspreload from https://files.pythonhosted.org/packages/b6/26/eaff7948f46de318ac3b86fc68d72106c41bcfdb46b77d55712c22565808/hstspreload-2024.6.1-py3-none-any.whl.metadata\n",
      "  Downloading hstspreload-2024.6.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\utilisateur\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.2.0)\n",
      "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Obtaining dependency information for chardet==3.* from https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl.metadata\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Obtaining dependency information for idna==2.* from https://files.pythonhosted.org/packages/a2/38/928ddce2273eaa564f6f50de919327bf3a00f091b5baba8dfa9460f3a8a8/idna-2.10-py2.py3-none-any.whl.metadata\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Obtaining dependency information for rfc3986<2,>=1.3 from https://files.pythonhosted.org/packages/c4/e5/63ca2c4edf4e00657584608bee1001302bbf8c5f569340b78304f2f446cb/rfc3986-1.5.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Obtaining dependency information for httpcore==0.9.* from https://files.pythonhosted.org/packages/dd/d5/e4ff9318693ac6101a2095e580908b591838c6f33df8d3ee8dd953ba96a8/httpcore-0.9.1-py3-none-any.whl.metadata\n",
      "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Obtaining dependency information for h11<0.10,>=0.8 from https://files.pythonhosted.org/packages/5a/fd/3dad730b0f95e78aeeb742f96fa7bbecbdd56a58e405d3da440d5bfb90c6/h11-0.9.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Obtaining dependency information for h2==3.* from https://files.pythonhosted.org/packages/25/de/da019bcc539eeab02f6d45836f23858ac467f584bfec7a526ef200242afe/h2-3.2.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Obtaining dependency information for hyperframe<6,>=5.2.0 from https://files.pythonhosted.org/packages/19/0c/bf88182bcb5dce3094e2f3e4fe20db28a9928cb7bd5b08024030e4b140db/hyperframe-5.2.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Obtaining dependency information for hpack<4,>=3.0 from https://files.pythonhosted.org/packages/8a/cc/e53517f4a1e13f74776ca93271caef378dadec14d71c61c949d759d3db69/hpack-3.0.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
      "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
      "   ---------------------------------------- 0.0/55.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 55.1/55.1 kB 2.8 MB/s eta 0:00:00\n",
      "Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "   ---------------------------------------- 0.0/133.4 kB ? eta -:--:--\n",
      "   ---------------------------------------  133.1/133.4 kB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 133.4/133.4 kB 2.6 MB/s eta 0:00:00\n",
      "Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
      "   ---------------------------------------- 0.0/42.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 42.6/42.6 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "   ---------------------------------------- 0.0/58.8 kB ? eta -:--:--\n",
      "   -------------------- ------------------- 30.7/58.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 58.8/58.8 kB 622.7 kB/s eta 0:00:00\n",
      "Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
      "   ---------------------------------------- 0.0/65.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 65.0/65.0 kB 1.7 MB/s eta 0:00:00\n",
      "Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
      "Downloading hstspreload-2024.6.1-py3-none-any.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.2/1.2 MB 5.3 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.3/1.2 MB 3.9 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.4/1.2 MB 3.9 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.5/1.2 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 0.7/1.2 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 0.8/1.2 MB 2.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 0.9/1.2 MB 2.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.0/1.2 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.2/1.2 MB 2.5 MB/s eta 0:00:00\n",
      "Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
      "   ---------------------------------------- 0.0/53.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 53.6/53.6 kB ? eta 0:00:00\n",
      "Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
      "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
      "Building wheels for collected packages: googletrans\n",
      "  Building wheel for googletrans (setup.py): started\n",
      "  Building wheel for googletrans (setup.py): finished with status 'done'\n",
      "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17461 sha256=e3b270eac8d2b0413e480ee8c54e608ea8a5023a0cd4b62460ff41a895cad939\n",
      "  Stored in directory: c:\\users\\utilisateur\\appdata\\local\\pip\\cache\\wheels\\39\\17\\6f\\66a045ea3d168826074691b4b787b8f324d3f646d755443fda\n",
      "Successfully built googletrans\n",
      "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
      "  Attempting uninstall: chardet\n",
      "    Found existing installation: chardet 4.0.0\n",
      "    Uninstalling chardet-4.0.0:\n",
      "      Successfully uninstalled chardet-4.0.0\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.4\n",
      "    Uninstalling idna-3.4:\n",
      "      Successfully uninstalled idna-3.4\n",
      "Successfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.6.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install googletrans==4.0.0-rc1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57485cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import f\n",
    "from nltk.corpus import names\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import string\n",
    "\n",
    "from googletrans import Translator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1dcdf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "134f7d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Utilisateur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Utilisateur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Utilisateur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Utilisateur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Utilisateur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Utilisateur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\Utilisateur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Téléchargement des ressources nécessaires de NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb12b2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fonction de prétraitement pour nettoyer le texte\n",
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # POS Tagging\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    # NER Tagging\n",
    "    ner_tags = ne_chunk(pos_tags)\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(pos_tag)) for token, pos_tag in pos_tags]\n",
    "    # Remove stop words and punctuation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token.lower() for token in lemmatized_tokens if token.lower() not in stop_words and token.lower() not in string.punctuation]\n",
    "    return filtered_tokens, ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74dffd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fonction pour obtenir la partie de la parole de WordNet\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b713bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fonction pour la modélisation des sujets\n",
    "def topic_modeling(texts, num_topics=3):\n",
    "    count_vectorizer = CountVectorizer(stop_words='english')\n",
    "    X = count_vectorizer.fit_transform(texts)\n",
    "    lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "    lda.fit(X)\n",
    "    return lda, count_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f2d8780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(text, target_language):\n",
    "    translator = Translator()\n",
    "    translated_text = translator.translate(text, dest=target_language)\n",
    "    return translated_text.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ada3859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87c21df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence, paragraph, or upload a file: hello sidali, how are you\n",
      "POS Tags:\n",
      "[('hello', 'NN'), ('sidali', 'NN')]\n",
      "\n",
      "NER Tags:\n",
      "(S hello/NN sidali/NN ,/, how/WRB are/VBP you/PRP)\n",
      "\n",
      "Top Topics:\n",
      "Topic 1:\n",
      "['sidali', 'hello']\n",
      "Topic 2:\n",
      "['sidali', 'hello']\n",
      "Topic 3:\n",
      "['hello', 'sidali']\n"
     ]
    }
   ],
   "source": [
    "# Interface utilisateur\n",
    "user_input = input(\"Enter a sentence, paragraph, or upload a file: \")\n",
    "\n",
    "# Prétraitement des données\n",
    "tokens, ner_tags = preprocess_text(user_input)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"POS Tags:\")\n",
    "print(pos_tag(tokens))\n",
    "print(\"\\nNER Tags:\")\n",
    "print(ner_tags)\n",
    "\n",
    "# Modélisation des sujets\n",
    "lda_model, count_vectorizer = topic_modeling([user_input])\n",
    "print(\"\\nTop Topics:\")\n",
    "for idx, topic in enumerate(lda_model.components_):\n",
    "    print(f\"Topic {idx + 1}:\")\n",
    "    feature_names = count_vectorizer.get_feature_names_out()\n",
    "    top_words_indices = topic.argsort()[-10:][::-1]\n",
    "    top_words = [feature_names[i] for i in top_words_indices]\n",
    "    print(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "feed76a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the target language (e.g., 'fr' for French): ar\n",
      "\n",
      "Translated Text:\n",
      "مرحبا سيدي ، كيف حالك\n"
     ]
    }
   ],
   "source": [
    "\n",
    "target_language = input(\"Enter the target language (e.g., 'fr' for French): \")\n",
    "\n",
    "# Traduction du texte\n",
    "translated_text = translate_text(user_input, target_language)\n",
    "print(\"\\nTranslated Text:\")\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eae998",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e49f7de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76c68c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7020d550",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
