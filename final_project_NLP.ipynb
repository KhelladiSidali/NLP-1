{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76056008",
   "metadata": {},
   "source": [
    "**KHELLADI Sid Ali**\n",
    "\n",
    "**DOUID Mohamed**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48685ac7",
   "metadata": {},
   "source": [
    "# I) Back-end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0477e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "# 1.1\n",
    "import xml.etree.ElementTree as ET\n",
    "# 1.2\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "# 2\n",
    "from collections import Counter, defaultdict\n",
    "# 4\n",
    "from gensim import corpora, models\n",
    "# 5\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9fff2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Utilisateur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6fab5a",
   "metadata": {},
   "source": [
    "**1) Prétraitement des Données**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baed4da4",
   "metadata": {},
   "source": [
    "**1.1) Extraction des \\<AbstractText\\>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b67ca984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_abstract_texts(file_path, max_abstracts=25):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    abstracts = []\n",
    "    \n",
    "    for i, abstract in enumerate(root.iter('AbstractText')):\n",
    "        if i < max_abstracts:\n",
    "            abstracts.append(abstract.text)\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7555b3",
   "metadata": {},
   "source": [
    "**1.2) Segmentation en phrases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4c6cc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_sentences(abstracts):\n",
    "    sentences = [sent_tokenize(abstract) for abstract in abstracts if isinstance(abstract, str)]\n",
    "    return [sentence for sublist in sentences for sentence in sublist]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e89285e",
   "metadata": {},
   "source": [
    "**1.3) Creation du fichier global**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4a35abcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_global_file(sentences, output_path):\n",
    "    # Ouverture du fichier avec l'encodage UTF-8\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for sentence in sentences:\n",
    "            f.write(sentence + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7e7c8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1\n",
    "abstracts = extract_abstract_texts(\"sample-0001.xml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2255c01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2\n",
    "sentences = segment_sentences(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a7cae61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3\n",
    "output_path = \"output.txt\"\n",
    "\n",
    "create_global_file(sentences, output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3265d238",
   "metadata": {},
   "source": [
    "**2) Etiquetage Morphosyntaxique (POS Tagging)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e489e105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_transition_matrices(tags):\n",
    "    unigram_counts = Counter(tags)\n",
    "    bigram_counts = Counter(zip(tags, tags[1:]))\n",
    "    \n",
    "    total_unigrams = sum(unigram_counts.values())\n",
    "    total_bigrams = sum(bigram_counts.values())\n",
    "\n",
    "    unigram_probs = {tag: count / total_unigrams for tag, count in unigram_counts.items()}\n",
    "    bigram_probs = {bigram: count / total_bigrams for bigram, count in bigram_counts.items()}\n",
    "\n",
    "    return unigram_probs, bigram_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6f743135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram_probs :  {'NOUN': 0.5, 'VERB': 0.25, 'ADJ': 0.25}\n",
      "bigram_probs :  {('NOUN', 'VERB'): 0.3333333333333333, ('VERB', 'NOUN'): 0.3333333333333333, ('NOUN', 'ADJ'): 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "myList = [\"NOUN\",\"VERB\",\"NOUN\",\"ADJ\"]\n",
    "unigram_probs, bigram_probs = compute_transition_matrices(myList)\n",
    "print(\"unigram_probs : \",unigram_probs)\n",
    "print(\"bigram_probs : \",bigram_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5460eb",
   "metadata": {},
   "source": [
    "**3) Reconnaissance d'Entités Nommées (NER)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e2499573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ner_transition_matrices(tags):\n",
    "    unigram_counts = Counter(tags)\n",
    "    bigram_counts = Counter(zip(tags, tags[1:]))\n",
    "\n",
    "    total_unigrams = sum(unigram_counts.values())\n",
    "    total_bigrams = sum(bigram_counts.values())\n",
    "\n",
    "    unigram_probs = {tag: count / total_unigrams for tag, count in unigram_counts.items()}\n",
    "    bigram_probs = {bigram: count / total_bigrams for bigram, count in bigram_counts.items()}\n",
    "\n",
    "    return unigram_probs, bigram_probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65717b04",
   "metadata": {},
   "source": [
    "**4) Modélisation des Sujets (Topics Modeling)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0a9eac78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_modeling(texts, num_topics=3):\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    ldamodel = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)\n",
    "    topics = ldamodel.print_topics(num_words=4)\n",
    "    return topics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adc5872",
   "metadata": {},
   "source": [
    "**5) Traduction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "91ff5616",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(text, src_lang='en', tgt_lang='fr'):\n",
    "    model_name = f'Helsinki-NLP/opus-mt-{src_lang}-{tgt_lang}'\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    translated = model.generate(**tokenizer(text, return_tensors=\"pt\", padding=True))\n",
    "    translation = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
    "    return translation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27d43e3",
   "metadata": {},
   "source": [
    "# II) Front-end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "403e3ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ea6a1d",
   "metadata": {},
   "source": [
    "**1) Interface Utilisateur**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11c2774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return 'Hello, World!'\n",
    "\n",
    "@app.route('/api/data', methods=['POST'])\n",
    "def get_data():\n",
    "    data = request.get_json()\n",
    "    response = {'received': data}\n",
    "    return jsonify(response)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        app.run(debug=True, use_reloader=False)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40db2361",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
