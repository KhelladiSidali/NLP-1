{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1e4a71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab94206",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7dda3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('words')\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('maxent_ne_chunker')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1d3c0be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"NLTK_DATA\"] = \"C:/Users/Utilisateur/AppData/Roaming/nltk_data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5a082364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "input_file = 'sample-0001.xml'\n",
    "output_file = 'sample.txt'\n",
    "translation_output_file = 'sample_translated.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d6afd2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Preprocessing the Dataset\n",
    "def extract_abstract_texts(file_path):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    abstracts = []\n",
    "    for abstract in root.findall(\".//AbstractText\"):\n",
    "        abstracts.append(abstract.text)\n",
    "    return abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "da416471",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def segment_sentences(abstracts):\n",
    "    sentences = []\n",
    "    for abstract in abstracts:\n",
    "        if abstract:\n",
    "            sentences.extend(sent_tokenize(abstract))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "54a66d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = extract_abstract_texts(input_file)\n",
    "sentences = segment_sentences(abstracts[:10])  # Adjust the number 10 based on your capacity\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for sentence in sentences:\n",
    "        f.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ed64206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Part of Speech Tagging\n",
    "\n",
    "def pos_tagging(sentences):\n",
    "    pos_tags = [nltk.pos_tag(word_tokenize(sentence)) for sentence in sentences]\n",
    "    return pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "43973699",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_transition_matrices(tags):\n",
    "    unigram_counts = Counter()\n",
    "    bigram_counts = Counter()\n",
    "    \n",
    "    for tag_seq in tags:\n",
    "        tag_seq = [tag for word, tag in tag_seq]  # Correcting the list comprehension to unpack tuples correctly\n",
    "        unigram_counts.update(tag_seq)\n",
    "        bigram_counts.update(nltk.bigrams(tag_seq))\n",
    "    \n",
    "    unigram_total = sum(unigram_counts.values())\n",
    "    bigram_total = sum(bigram_counts.values())\n",
    "    \n",
    "    unigram_probs = {tag: count/unigram_total for tag, count in unigram_counts.items()}\n",
    "    bigram_probs = {bigram: count/bigram_total for bigram, count in bigram_counts.items()}\n",
    "    \n",
    "    return unigram_probs, bigram_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dfd8d4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Part of Speech Tagging Unigram Probabilities:\n",
      "{'DT': 0.0889894419306184, 'JJ': 0.11161387631975868, 'NN': 0.16515837104072398, 'NNP': 0.05505279034690799, 'IN': 0.12518853695324283, 'NNS': 0.07993966817496229, 'VBD': 0.033182503770739065, '(': 0.014328808446455505, ')': 0.014328808446455505, 'CD': 0.04524886877828054, 'TO': 0.016591251885369532, 'VB': 0.016591251885369532, 'CC': 0.029411764705882353, ',': 0.03695324283559578, '.': 0.03770739064856712, 'VBN': 0.033936651583710405, 'NNPS': 0.0015082956259426848, 'VBP': 0.00980392156862745, 'RB': 0.02262443438914027, 'VBG': 0.008295625942684766, 'JJR': 0.004524886877828055, 'VBZ': 0.017345399698340876, '``': 0.0007541478129713424, \"''\": 0.0007541478129713424, 'PRP': 0.00904977375565611, 'WP': 0.0015082956259426848, 'RBR': 0.0007541478129713424, '$': 0.0007541478129713424, 'MD': 0.0022624434389140274, 'WDT': 0.005279034690799397, 'WRB': 0.0015082956259426848, 'EX': 0.0022624434389140274, ':': 0.0030165912518853697, 'PRP$': 0.0022624434389140274, 'JJS': 0.0007541478129713424, 'POS': 0.0007541478129713424}\n"
     ]
    }
   ],
   "source": [
    "pos_tags = pos_tagging(sentences)\n",
    "pos_unigram_probs, pos_bigram_probs = compute_transition_matrices(pos_tags)\n",
    "\n",
    "print(\"### Part of Speech Tagging Unigram Probabilities:\")\n",
    "print(pos_unigram_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6366f880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Part of Speech Tagging Bigram Probabilities:\n",
      "{('DT', 'JJ'): 0.0219435736677116, ('JJ', 'NN'): 0.05172413793103448, ('NN', 'NN'): 0.019592476489028215, ('NN', 'NNP'): 0.009404388714733543, ('NNP', 'NNP'): 0.0109717868338558, ('NNP', 'IN'): 0.0054858934169279, ('IN', 'NNS'): 0.006269592476489028, ('NNS', 'IN'): 0.025078369905956112, ('IN', 'NN'): 0.024294670846394983, ('NN', 'VBD'): 0.006269592476489028, ('VBD', 'IN'): 0.003918495297805642, ('IN', 'DT'): 0.039968652037617555, ('DT', 'NNP'): 0.007053291536050157, ('IN', 'NNP'): 0.007836990595611285, ('NNP', '('): 0.003918495297805642, ('(', 'NNP'): 0.009404388714733543, ('NNP', ')'): 0.0054858934169279, (')', 'IN'): 0.001567398119122257, ('NNP', 'CD'): 0.0054858934169279, ('CD', 'TO'): 0.0023510971786833857, ('TO', 'VB'): 0.009404388714733543, ('VB', 'DT'): 0.006269592476489028, ('DT', 'NNS'): 0.0109717868338558, ('NN', 'CC'): 0.0109717868338558, ('CC', 'JJ'): 0.006269592476489028, ('JJ', 'JJ'): 0.014890282131661442, ('NN', 'IN'): 0.05642633228840126, ('IN', 'CD'): 0.011755485893416929, ('CD', 'NNS'): 0.014106583072100314, ('NNS', ','): 0.009404388714733543, (',', 'NNS'): 0.0007836990595611285, (',', 'CC'): 0.011755485893416929, ('JJ', 'NNS'): 0.0274294670846395, ('NNS', '.'): 0.014106583072100314, ('NNS', 'VBD'): 0.01018808777429467, ('VBD', 'VBN'): 0.01018808777429467, ('VBN', 'IN'): 0.013322884012539185, ('NNP', '.'): 0.0023510971786833857, ('TO', 'CD'): 0.0007836990595611285, ('CD', 'IN'): 0.0023510971786833857, ('DT', 'NN'): 0.03918495297805643, ('IN', 'NNPS'): 0.0007836990595611285, ('NNPS', 'TO'): 0.0007836990595611285, ('TO', 'NNPS'): 0.0007836990595611285, ('NNPS', '.'): 0.0007836990595611285, ('IN', 'JJ'): 0.03213166144200627, (',', 'CD'): 0.008620689655172414, ('DT', 'CD'): 0.0023510971786833857, ('NNS', 'CC'): 0.003134796238244514, ('NNS', 'VBP'): 0.0054858934169279, ('VBP', 'JJ'): 0.003134796238244514, ('JJ', 'CD'): 0.0007836990595611285, ('VBP', 'RB'): 0.003134796238244514, ('RB', 'JJ'): 0.0054858934169279, ('JJ', 'IN'): 0.004702194357366771, (',', 'IN'): 0.004702194357366771, ('VBN', 'RB'): 0.0023510971786833857, ('JJ', 'TO'): 0.004702194357366771, ('NN', '.'): 0.012539184952978056, ('VB', 'VBG'): 0.0007836990595611285, ('VBG', 'NNS'): 0.001567398119122257, ('NN', ','): 0.0109717868338558, (',', 'DT'): 0.003134796238244514, ('VBD', 'RB'): 0.003134796238244514, ('RB', 'VBN'): 0.007053291536050157, ('VBN', ','): 0.001567398119122257, ('NNS', 'VBG'): 0.0023510971786833857, ('VBG', 'DT'): 0.003134796238244514, ('CC', 'CD'): 0.0023510971786833857, ('DT', 'JJR'): 0.001567398119122257, ('JJR', 'NN'): 0.001567398119122257, ('DT', 'VBG'): 0.0007836990595611285, ('VBG', 'CD'): 0.0007836990595611285, ('VBN', 'JJ'): 0.001567398119122257, ('NN', 'VBZ'): 0.006269592476489028, ('VBZ', 'DT'): 0.006269592476489028, (',', 'NN'): 0.001567398119122257, ('NN', 'VBG'): 0.001567398119122257, ('NNS', 'VBZ'): 0.001567398119122257, ('VBZ', 'VBN'): 0.0023510971786833857, ('DT', '``'): 0.0007836990595611285, ('``', 'NNP'): 0.0007836990595611285, ('.', \"''\"): 0.0007836990595611285, ('PRP', 'VBD'): 0.004702194357366771, ('VBD', 'DT'): 0.006269592476489028, ('TO', 'JJ'): 0.001567398119122257, ('NNS', 'WP'): 0.001567398119122257, ('WP', 'VBD'): 0.0007836990595611285, ('JJ', ','): 0.0023510971786833857, ('VBD', '('): 0.0007836990595611285, ('(', 'CD'): 0.0023510971786833857, ('CD', 'NN'): 0.008620689655172414, ('NN', ')'): 0.003134796238244514, (')', '.'): 0.003918495297805642, ('VBD', 'VBG'): 0.0007836990595611285, ('VBG', 'NN'): 0.0007836990595611285, ('VBD', 'JJR'): 0.001567398119122257, ('JJR', ','): 0.0007836990595611285, (',', 'VBD'): 0.001567398119122257, ('VBD', 'RBR'): 0.0007836990595611285, ('RBR', 'JJ'): 0.0007836990595611285, ('NN', 'NNS'): 0.011755485893416929, ('CC', 'VBD'): 0.0023510971786833857, ('VBN', 'TO'): 0.001567398119122257, ('VB', 'IN'): 0.0007836990595611285, ('NN', '('): 0.0054858934169279, ('(', 'JJ'): 0.001567398119122257, ('JJ', '$'): 0.0007836990595611285, ('$', 'CD'): 0.0007836990595611285, ('NNS', ')'): 0.0007836990595611285, (')', 'VBD'): 0.0023510971786833857, ('NNP', 'NN'): 0.007836990595611285, ('CC', 'DT'): 0.006269592476489028, ('RB', 'IN'): 0.001567398119122257, ('NN', 'MD'): 0.0007836990595611285, ('MD', 'VB'): 0.0023510971786833857, ('VB', 'VBN'): 0.0007836990595611285, ('RB', 'RB'): 0.0023510971786833857, ('JJ', '.'): 0.0023510971786833857, ('DT', 'RB'): 0.0007836990595611285, ('VBN', 'NN'): 0.0023510971786833857, ('NNS', '('): 0.0023510971786833857, ('NN', 'CD'): 0.001567398119122257, ('CD', ','): 0.008620689655172414, ('NN', 'JJ'): 0.007053291536050157, (',', 'RB'): 0.001567398119122257, ('RB', 'JJR'): 0.0007836990595611285, ('JJR', 'IN'): 0.001567398119122257, ('CD', ')'): 0.003134796238244514, ('NN', 'VBP'): 0.0007836990595611285, ('VBP', 'IN'): 0.0007836990595611285, ('IN', 'PRP'): 0.0007836990595611285, ('PRP', 'MD'): 0.0007836990595611285, ('VB', 'JJ'): 0.001567398119122257, ('VB', 'NN'): 0.0007836990595611285, ('CC', 'NNS'): 0.001567398119122257, ('NNS', 'WDT'): 0.001567398119122257, ('WDT', 'VBZ'): 0.004702194357366771, ('VBN', 'DT'): 0.0023510971786833857, ('DT', 'IN'): 0.0023510971786833857, ('CC', 'RB'): 0.001567398119122257, ('RB', '.'): 0.0007836990595611285, ('PRP', 'RB'): 0.001567398119122257, ('RB', 'VBD'): 0.0023510971786833857, ('VBD', 'NNS'): 0.0007836990595611285, ('CC', 'PRP'): 0.0007836990595611285, ('CC', 'WRB'): 0.0007836990595611285, ('WRB', 'EX'): 0.0007836990595611285, ('EX', 'VBZ'): 0.001567398119122257, ('VBZ', 'JJ'): 0.0023510971786833857, ('VBN', 'NNS'): 0.0007836990595611285, ('VBN', 'CC'): 0.0007836990595611285, ('CC', 'VBN'): 0.0007836990595611285, ('CC', 'NN'): 0.003918495297805642, ('NNP', 'JJ'): 0.003134796238244514, ('NNS', ':'): 0.001567398119122257, (':', 'DT'): 0.0007836990595611285, ('NNP', 'VBD'): 0.001567398119122257, ('JJ', 'CC'): 0.0007836990595611285, ('VBD', 'JJ'): 0.003134796238244514, ('DT', 'CC'): 0.0007836990595611285, ('CD', 'JJ'): 0.004702194357366771, ('NNS', 'CD'): 0.0007836990595611285, ('DT', 'VBD'): 0.001567398119122257, (',', 'VBG'): 0.0007836990595611285, ('VBG', 'IN'): 0.0007836990595611285, ('VBZ', 'RB'): 0.004702194357366771, ('IN', 'PRP$'): 0.001567398119122257, ('PRP$', 'NN'): 0.0023510971786833857, ('NNP', 'NNS'): 0.004702194357366771, ('DT', 'VBZ'): 0.0007836990595611285, ('NN', 'WDT'): 0.0023510971786833857, ('IN', 'VBN'): 0.0007836990595611285, (')', 'VBZ'): 0.001567398119122257, ('NN', 'VBN'): 0.0007836990595611285, ('NN', 'TO'): 0.003918495297805642, ('VB', 'NNP'): 0.0007836990595611285, ('NNP', 'VBG'): 0.0007836990595611285, ('NNS', 'JJ'): 0.0007836990595611285, ('NNP', ','): 0.001567398119122257, ('NNP', 'VBZ'): 0.001567398119122257, ('IN', 'JJS'): 0.0007836990595611285, ('JJS', 'CD'): 0.0007836990595611285, ('TO', 'NNP'): 0.001567398119122257, (')', 'VBG'): 0.0007836990595611285, ('VBG', 'VBN'): 0.0007836990595611285, ('VBN', 'VBN'): 0.004702194357366771, ('VBN', '.'): 0.0023510971786833857, ('VBD', 'TO'): 0.001567398119122257, ('VB', 'CD'): 0.003918495297805642, ('CD', '('): 0.001567398119122257, ('(', 'VB'): 0.001567398119122257, (')', ','): 0.0023510971786833857, ('CC', 'VBP'): 0.0007836990595611285, ('VBP', 'VBN'): 0.001567398119122257, ('TO', 'DT'): 0.003134796238244514, ('CC', 'MD'): 0.0007836990595611285, ('VB', 'VB'): 0.0007836990595611285, ('NNS', 'VBN'): 0.001567398119122257, ('VBZ', 'NN'): 0.0007836990595611285, ('DT', 'VBN'): 0.001567398119122257, ('IN', 'RB'): 0.0007836990595611285, ('RB', 'CD'): 0.0007836990595611285, ('NNS', 'PRP'): 0.0007836990595611285, ('PRP', 'VBP'): 0.0023510971786833857, ('VBN', 'CD'): 0.0007836990595611285, ('WP', 'RB'): 0.0007836990595611285, ('VBD', 'NNP'): 0.0007836990595611285, ('VBP', 'DT'): 0.001567398119122257, (',', 'PRP'): 0.0007836990595611285, (':', 'NN'): 0.0023510971786833857, ('JJ', ')'): 0.0023510971786833857, (',', 'JJ'): 0.0007836990595611285, ('RB', ','): 0.0007836990595611285, ('VBN', '('): 0.0007836990595611285, (')', 'CC'): 0.0007836990595611285, ('IN', 'IN'): 0.0007836990595611285, ('EX', 'VBD'): 0.0007836990595611285, ('IN', 'JJR'): 0.0007836990595611285, ('JJR', 'NNS'): 0.0007836990595611285, ('NNP', 'CC'): 0.001567398119122257, ('RB', 'VB'): 0.001567398119122257, ('JJ', 'NNP'): 0.0023510971786833857, ('VBG', 'JJ'): 0.0007836990595611285, ('JJ', 'VBN'): 0.0007836990595611285, ('NNP', 'POS'): 0.0007836990595611285, ('POS', 'NN'): 0.0007836990595611285, ('CD', ':'): 0.001567398119122257, (')', 'NN'): 0.001567398119122257, (',', 'WDT'): 0.0007836990595611285, ('VBZ', 'TO'): 0.0007836990595611285, (',', 'VBP'): 0.0007836990595611285, ('CC', 'VB'): 0.001567398119122257, ('IN', 'WDT'): 0.0007836990595611285, ('WDT', 'NN'): 0.0007836990595611285, (',', 'NNP'): 0.0007836990595611285, ('CC', 'NNP'): 0.0007836990595611285, ('VBD', 'CD'): 0.0007836990595611285, ('VB', 'WRB'): 0.0007836990595611285, ('WRB', 'RB'): 0.0007836990595611285, ('VBZ', 'IN'): 0.0007836990595611285}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n### Part of Speech Tagging Bigram Probabilities:\")\n",
    "print(pos_bigram_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "83ca9b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Named Entity Recognition\n",
    "\n",
    "def ner_tagging(sentences):\n",
    "    ner_tags = []\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        tagged_words = nltk.pos_tag(words)\n",
    "        chunks = nltk.ne_chunk(tagged_words)\n",
    "        for chunk in chunks:\n",
    "            if hasattr(chunk, 'label'):\n",
    "                ner_tags.append(chunk.label())\n",
    "            else:\n",
    "                ner_tags.append(chunk[1])\n",
    "    return ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d9df2ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_ner_transition_matrices(ner_tags):\n",
    "    unigram_counts = Counter()\n",
    "    bigram_counts = Counter()\n",
    "    \n",
    "    unigram_counts.update(ner_tags)\n",
    "    bigram_counts.update(nltk.bigrams(ner_tags))\n",
    "    \n",
    "    unigram_total = sum(unigram_counts.values())\n",
    "    bigram_total = sum(bigram_counts.values())\n",
    "    \n",
    "    unigram_probs = {tag: count/unigram_total for tag, count in unigram_counts.items()}\n",
    "    bigram_probs = {bigram: count/bigram_total for bigram, count in bigram_counts.items()}\n",
    "    \n",
    "    return unigram_probs, bigram_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a515adb7",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('words')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/words\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Utilisateur/nltk_data'\n    - 'C:\\\\Users\\\\Utilisateur\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Utilisateur\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Utilisateur\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Utilisateur\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('words')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/words.zip/words/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Utilisateur/nltk_data'\n    - 'C:\\\\Users\\\\Utilisateur\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Utilisateur\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Utilisateur\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Utilisateur\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ner_tags \u001b[38;5;241m=\u001b[39m ner_tagging(sentences)\n\u001b[0;32m      2\u001b[0m ner_unigram_probs, ner_bigram_probs \u001b[38;5;241m=\u001b[39m compute_ner_transition_matrices(ner_tags)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m### Named Entity Recognition Unigram Probabilities:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[51], line 8\u001b[0m, in \u001b[0;36mner_tagging\u001b[1;34m(sentences)\u001b[0m\n\u001b[0;32m      6\u001b[0m words \u001b[38;5;241m=\u001b[39m word_tokenize(sentence)\n\u001b[0;32m      7\u001b[0m tagged_words \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mpos_tag(words)\n\u001b[1;32m----> 8\u001b[0m chunks \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mne_chunk(tagged_words)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(chunk, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\chunk\\__init__.py:184\u001b[0m, in \u001b[0;36mne_chunk\u001b[1;34m(tagged_tokens, binary)\u001b[0m\n\u001b[0;32m    182\u001b[0m     chunker_pickle \u001b[38;5;241m=\u001b[39m _MULTICLASS_NE_CHUNKER\n\u001b[0;32m    183\u001b[0m chunker \u001b[38;5;241m=\u001b[39m load(chunker_pickle)\n\u001b[1;32m--> 184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chunker\u001b[38;5;241m.\u001b[39mparse(tagged_tokens)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\chunk\\named_entity.py:127\u001b[0m, in \u001b[0;36mNEChunkParser.parse\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens):\n\u001b[0;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;124;03m    Each token should be a pos-tagged word\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 127\u001b[0m     tagged \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tagger\u001b[38;5;241m.\u001b[39mtag(tokens)\n\u001b[0;32m    128\u001b[0m     tree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tagged_to_parse(tagged)\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tree\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tag\\sequential.py:61\u001b[0m, in \u001b[0;36mSequentialBackoffTagger.tag\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m     59\u001b[0m tags \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tokens)):\n\u001b[1;32m---> 61\u001b[0m     tags\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtag_one(tokens, i, tags))\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(tokens, tags))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tag\\sequential.py:81\u001b[0m, in \u001b[0;36mSequentialBackoffTagger.tag_one\u001b[1;34m(self, tokens, index, history)\u001b[0m\n\u001b[0;32m     79\u001b[0m tag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tagger \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_taggers:\n\u001b[1;32m---> 81\u001b[0m     tag \u001b[38;5;241m=\u001b[39m tagger\u001b[38;5;241m.\u001b[39mchoose_tag(tokens, index, history)\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tag \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tag\\sequential.py:647\u001b[0m, in \u001b[0;36mClassifierBasedTagger.choose_tag\u001b[1;34m(self, tokens, index, history)\u001b[0m\n\u001b[0;32m    645\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchoose_tag\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens, index, history):\n\u001b[0;32m    646\u001b[0m     \u001b[38;5;66;03m# Use our feature detector to get the featureset.\u001b[39;00m\n\u001b[1;32m--> 647\u001b[0m     featureset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_detector(tokens, index, history)\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;66;03m# Use the classifier to pick a tag.  If a cutoff probability\u001b[39;00m\n\u001b[0;32m    650\u001b[0m     \u001b[38;5;66;03m# was specified, then check that the tag's probability is\u001b[39;00m\n\u001b[0;32m    651\u001b[0m     \u001b[38;5;66;03m# higher than that cutoff first; otherwise, return None.\u001b[39;00m\n\u001b[0;32m    652\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cutoff_prob \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tag\\sequential.py:694\u001b[0m, in \u001b[0;36mClassifierBasedTagger.feature_detector\u001b[1;34m(self, tokens, index, history)\u001b[0m\n\u001b[0;32m    684\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeature_detector\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens, index, history):\n\u001b[0;32m    685\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    686\u001b[0m \u001b[38;5;124;03m    Return the feature detector that this tagger uses to generate\u001b[39;00m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;124;03m    featuresets for its classifier.  The feature detector is a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[38;5;124;03m    See ``classifier()``\u001b[39;00m\n\u001b[0;32m    693\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 694\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_detector(tokens, index, history)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\chunk\\named_entity.py:101\u001b[0m, in \u001b[0;36mNEChunkParserTagger._feature_detector\u001b[1;34m(self, tokens, index, history)\u001b[0m\n\u001b[0;32m     90\u001b[0m     nextnextpos \u001b[38;5;241m=\u001b[39m tokens[index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# 89.6\u001b[39;00m\n\u001b[0;32m     93\u001b[0m features \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m: shape(word),\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwordlen\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(word),\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefix3\u001b[39m\u001b[38;5;124m\"\u001b[39m: word[:\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuffix3\u001b[39m\u001b[38;5;124m\"\u001b[39m: word[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m:]\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m\"\u001b[39m: pos,\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m\"\u001b[39m: word,\n\u001b[1;32m--> 101\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men-wordlist\u001b[39m\u001b[38;5;124m\"\u001b[39m: (word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_english_wordlist()),\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprevtag\u001b[39m\u001b[38;5;124m\"\u001b[39m: prevtag,\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprevpos\u001b[39m\u001b[38;5;124m\"\u001b[39m: prevpos,\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnextpos\u001b[39m\u001b[38;5;124m\"\u001b[39m: nextpos,\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprevword\u001b[39m\u001b[38;5;124m\"\u001b[39m: prevword,\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnextword\u001b[39m\u001b[38;5;124m\"\u001b[39m: nextword,\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword+nextpos\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m+\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnextpos\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpos+prevtag\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpos\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m+\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprevtag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape+prevtag\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprevshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m+\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprevtag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    110\u001b[0m }\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\chunk\\named_entity.py:52\u001b[0m, in \u001b[0;36mNEChunkParserTagger._english_wordlist\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m words\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_en_wordlist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(words\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men-basic\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     53\u001b[0m     wl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_en_wordlist\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wl\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__load()\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('words')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/words\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Utilisateur/nltk_data'\n    - 'C:\\\\Users\\\\Utilisateur\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Utilisateur\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Utilisateur\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Utilisateur\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "ner_tags = ner_tagging(sentences)\n",
    "ner_unigram_probs, ner_bigram_probs = compute_ner_transition_matrices(ner_tags)\n",
    "\n",
    "print(\"\\n### Named Entity Recognition Unigram Probabilities:\")\n",
    "print(ner_unigram_probs)\n",
    "print(\"\\n### Named Entity Recognition Bigram Probabilities:\")\n",
    "print(ner_bigram_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900b91d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Topics Extraction Using TF-IDF for Each Abstract\n",
    "def extract_topics_for_each_abstract(abstracts, num_topics=3, num_words=5):\n",
    "    topics_per_abstract = []\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=100)\n",
    "\n",
    "    for abstract in abstracts:\n",
    "        if abstract:  # Check if the abstract is not None or empty\n",
    "            tfidf_matrix = vectorizer.fit_transform([abstract])\n",
    "            feature_names = vectorizer.get_feature_names_out()\n",
    "            \n",
    "            # Sum the TF-IDF scores for each term in the abstract\n",
    "            tfidf_scores = tfidf_matrix.sum(axis=0).A1\n",
    "            tfidf_scores = dict(zip(feature_names, tfidf_scores))\n",
    "            \n",
    "            # Sort terms by their TF-IDF scores in descending order\n",
    "            sorted_terms = sorted(tfidf_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            # Extract the top terms as topics for the current abstract\n",
    "            topics = [term for term, score in sorted_terms[:num_topics]]\n",
    "            topics_per_abstract.append(topics)\n",
    "        else:\n",
    "            topics_per_abstract.append([])  # Handle empty abstracts\n",
    "\n",
    "    return topics_per_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b344532d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topics_per_abstract = extract_topics_for_each_abstract(abstracts[:10])  # Adjust the number 10 based on your capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4df2a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n### Topics per Abstract:\")\n",
    "for i, topics in enumerate(topics_per_abstract, 1):\n",
    "    print(f\"Abstract {i}: {topics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db4ce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 5: Translation\n",
    "def translate_sentences(sentences, target_language='fr'):\n",
    "    if target_language == 'fr':\n",
    "        translator = pipeline('translation_en_to_fr', model='Helsinki-NLP/opus-mt-en-fr')\n",
    "    elif target_language == 'ar':\n",
    "        translator = pipeline('translation_en_to_ar', model='Helsinki-NLP/opus-mt-en-ar')\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported target language. Use 'fr' for French or 'ar' for Arabic.\")\n",
    "    \n",
    "    translations = translator(sentences)\n",
    "    translated_sentences = [t['translation_text'] for t in translations]\n",
    "    return translated_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6cb127",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example: Translate to French\n",
    "translated_sentences = translate_sentences(sentences, target_language='fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eda6d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(translation_output_file, 'w', encoding='utf-8') as f:\n",
    "    for sentence in translated_sentences:\n",
    "        f.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62280a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"\\n### Translated Sentences:\")\n",
    "for i, sentence in enumerate(translated_sentences, 1):\n",
    "    print(f\"Sentence {i}: {sentence}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416a083b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7576d66d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
